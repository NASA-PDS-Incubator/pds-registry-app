<?xml version="1.0" encoding="UTF-8"?>

<document>
<properties>
  <title>Load Data (Scalable Harvest)</title>
</properties>

<body>

<section name="Load Data into Registry (Scalable Harvest)">

<p>
<ul>
  <li><a href="#Overview">Overview</a></li>
  <li><a href="#Prerequisites">Prerequisites</a></li>
  <li><a href="#quick">Quick Start</a></li>
  <li><a href="#NodeName">Node Name</a></li>
  <li><a href="#InputDirs">Input Directories and Filters</a></li>
  <ul>
    <li><a href="#Dirs">Process Directories</a></li>
    <li><a href="#Files">Process Files</a></li>
    <li><a href="#ProdClass">Filtering Products by Class</a></li>
  </ul>
</ul>
</p>
</section>

<section name="Overview">

<p>
To load PDS4 data into Registry you have to use Harvest software. There are two versions of Harvest:
a simple standalone command-line tool and a scalable Harvest consisting of several server and and
client components. Scalable Harvest can process big data sets in parallel. 
Both versions extract metadata from PDS4 products (labels) and load extracted
metadata into Elasticsearch database. 
</p>
<p>
This document describes how to use Harvest Client command-line tool to submit jobs to Scalable Harvest Server cluster.
</p>

</section>


<section name="Prerequisites">

<ul>
  <li>Elasticsearch server is running.</li>
  <li>Registry indices are created in Elasticsearch.</li>
  <li>All server components - RabbitMQ, Crawler Server, Harvest Server - are deployed and running on-prem or in the cloud.</li>
  <li>Harvest Client command-line tool is installed.</li>
</ul>

</section>


<!-- ========================================================== -->

<section name="Scalable Harvest Quick Start" id="harvest_quick">
<p>
Scalable Harvest consists of several server components: RabbitMQ, Crawler and Harvest servers.
To load data you have to use Harvest Client command-line tool to submit a job to Harvest server cluster.
</p>

<h4>Configuration File</h4>
<p>
Harvest Client requires message broker (RabbitMQ) connection to submit jobs to the Harvest server cluster.
Default configuration file, <i>&lt;INSTALL_DIR&gt;/conf/harvest-client.cfg</i>, has the following parameters:
</p>

<source>
# Message server type. Currently, only 'RabbitMQ' is supported.
mq.type = RabbitMQ

# RabbitMQ host(s). One or more host:port tuples (one tuple per line).
# rmq.host = host1:5672
# rmq.host = host2:5672
# rmq.host = host3:5672

rmq.host = localhost:5672

# RabbitMQ user
rmq.user = harvest

# RabbitMQ password
rmq.password = harvest
</source>

<p>
You can either edit default configuration file, or create another file and pass it to Harvest Client CLI as a parameter.
</p>


<h4>Submit a Job</h4>

<p>
To submit a job to the harvest server cluster you need a job configuration file. 
An example configuration file is available in the installation directory:
<i>&lt;INSTALL_DIR&gt;/examples/directories.xml</i>. 
</p>

<p>
You will need to update the nodeName:
<source>
&lt;harvest nodeName="PDS_GEO"&gt;
</source>

The path to the data. 
NOTE: Crawler and Harvest servers should be able to read this path. 
<source>
  &lt;directories&gt;
    &lt;path&gt;/data/geo/urn-nasa-pds-kaguya_grs_spectra&lt;/path&gt;
  &lt;/directories&gt;
</source>
  
And the URL prefix for the data:
<source>
  &lt;fileInfo&gt;
    &lt;!-- UPDATE with your own local path and base url where pds4 archive is published --&gt;
    &lt;fileRef replacePrefix="/data/geo/" with="https://pds-geosciences.wustl.edu/lunar/" /&gt;
  &lt;/fileInfo&gt;
</source>
</p>

<p>
If you save this file as <i>/tmp/job1.xml</i> and run Harvest Client
<source>
&lt;INSTALL_DIR&gt;/bin/harvest-client harvest -j /tmp/job1.xml
</source>
</p>
<p>
You should see output similar to this:
<source>
[INFO] Reading job from /tmp/job1.xml
[INFO] Reading configuration from /tmp/big-data-harvest-client-1.0.0-SNAPSHOT/conf/harvest-client.cfg
[INFO] Creating new job...
[INFO] Connecting to RabbitMQ
[INFO] Created job f282a012-115e-429c-b445-f5eed1d81303
</source>
</p>

<p>
After submitting a job, you can monitor progress by querying Elasticsearch
<source>
curl "http://localhost:9200/registry/_search?q=_package_id:f282a012-115e-429c-b445-f5eed1d81303"
</source>
NOTE: For backward compatibility, job ID field is called "_package_id" in Elasticsearch.
</p>

<p>
The following sections describe job configuration file in more detail.
</p>

</section>

<!-- ====================================================================== -->

<section name="Node Name" id="NodeName">
<p>
Node name is a required parameter which is used to tag ingested data with the node it is ingested by.
</p>

<source>
&lt;harvest nodeName="PDS_SBN"&gt;
...
</source>

<p>
One of the following values can be used:
<ul>
  <li><b>PDS_ATM</b>  - Planetary Data System: Atmospheres Node</li>
  <li><b>PDS_ENG</b>  - Planetary Data System: Engineering Node</li>
  <li><b>PDS_GEO</b>  - Planetary Data System: Geosciences Node</li>
  <li><b>PDS_IMG</b>  - Planetary Data System: Imaging Node</li>
  <li><b>PDS_NAIF</b> - Planetary Data System: NAIF Node</li>
  <li><b>PDS_RMS</b>  - Planetary Data System: Rings Node</li>
  <li><b>PDS_SBN</b>  - Planetary Data System: Small Bodies Node at University of Maryland</li>
  <li><b>PSA</b>      - Planetary Science Archive</li>
  <li><b>JAXA</b>     - Japan Aerospace Exploration Agency</li>
  <li><b>ROSCOSMOS</b> - Russian State Corporation for Space Activities</li>
</ul>
</p>

<p>
This value is saved in "ops:Harvest_Info/ops:node_name" field in Elasticsearch document:
</p>
<source>
{
...
  "ops:Harvest_Info/ops:node_name": "PDS_SBN",
...
}
</source>

</section>

<!-- ====================================================================== -->

<section name="Input Directories and Filters" id="InputDirs">

<subsection name="Process Directories" id="Dirs">
<p>
To process products from one or more directories, add the following section in Harvest configuration file:
</p>
<source>
&lt;harvest nodeName="PDS_SBN"&gt;
  ...
  &lt;directories&gt;
    &lt;path&gt;/some-directory/sub-dir-1/&lt;/path&gt;
    &lt;path&gt;/some-directory/sub-dir-2/&lt;/path&gt;
  &lt;/directories&gt;
  ...
&lt;/harvest&gt;
</source>

<p>
<b>NOTE:</b> Crawler and Harvest server should be able to read these paths.
</p>

</subsection>


<subsection name="Process a List of Files" id="Files">
<p>
First, create a manifest file and list all files you want to process. One file path per line.
</p>
<source>
/data/d1/CCF_0088_0674757853_190FDR_N0040048CACH00100_0A10LLJ05.xml
/data/d1/CCF_0088_0674757853_190FDR_N0040048CACH00100_0A10LLJ07.xml
/data/d1/CCF_0088_0674757853_190FDR_N0040048CACH00100_0A10LLJ09.xml
</source>

<p>
Next, add the following section in Harvest configuration file:
</p>
<source>
&lt;harvest nodeName="PDS_SBN"&gt;
  ...
  &lt;files&gt;
    &lt;manifest&gt;/some-directory/manifest.txt/&lt;/manifest&gt;
  &lt;/files&gt;
  ...
&lt;/harvest&gt;
</source>

<p>
<b>NOTE:</b> Crawler and Harvest server should be able to read these paths (including manifest file).
</p>

</subsection>

<!-- ====================================================== -->

<subsection name="Filtering Products by Class" id="ProdClass">
<p>
You can include or exclude products of a particular class. For example, to only process documents, add following 
product filter in Harvest configuration file:
</p>
<source>
&lt;harvest nodeName="PDS_SBN"&gt;
  ...
  &lt;productFilter&gt;
    &lt;includeClass&gt;Product_Document&lt;/includeClass&gt;
  &lt;/productFilter&gt;
  ...
&lt;/harvest&gt;
</source>

<p>
To exclude documents, add following product filter:
</p>
<source>
&lt;harvest nodeName="PDS_SBN"&gt;
  ...
  &lt;productFilter&gt;
    &lt;excludeClass&gt;Product_Document&lt;/excludeClass&gt;
  &lt;/productFilter&gt;
  ...
&lt;/harvest&gt;
</source>

<p>
<b>NOTE:</b> You could not have both include and exclude filters at the same time.
</p>

</subsection>

</section>


</body>
</document>
